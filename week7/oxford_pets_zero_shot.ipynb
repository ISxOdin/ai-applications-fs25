{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from torch) (78.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilias\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 13.6/204.1 MB 71.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 38.5/204.1 MB 94.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 53.7/204.1 MB 90.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 70.3/204.1 MB 84.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 95.2/204.1 MB 90.6 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 111.7/204.1 MB 88.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 136.3/204.1 MB 92.6 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 161.2/204.1 MB 95.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 176.7/204.1 MB 92.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.5/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 94.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 57.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 63.1 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 45.7 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilias\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ilias\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"openai/clip-vit-large-patch14\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-image-classification\")\n",
    "#checkpoint = \"google/siglip-so400m-patch14-384\"\n",
    "#detector = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpcuenq/oxford-pets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('pcuenq/oxford-pets')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels_oxford_pets = ['Siamese', 'Birman', 'shiba inu', 'staffordshire bull terrier', 'basset hound', 'Bombay', 'japanese chin', 'chihuahua', 'german shorthaired', 'pomeranian', 'beagle', 'english cocker spaniel', 'american pit bull terrier', 'Ragdoll', 'Persian', 'Egyptian Mau', 'miniature pinscher', 'Sphynx', 'Maine Coon', 'keeshond', 'yorkshire terrier', 'havanese', 'leonberger', 'wheaten terrier', 'american bulldog', 'english setter', 'boxer', 'newfoundland', 'Bengal', 'samoyed', 'British Shorthair', 'great pyrenees', 'Abyssinian', 'pug', 'saint bernard', 'Russian Blue', 'scottish terrier']\n",
    "\n",
    "# List to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(len(dataset['train']))):\n",
    "    # Get the image bytes from the dataset\n",
    "    image_bytes = dataset['train'][i]['image']['bytes']\n",
    "    \n",
    "    # Convert the bytes to a PIL image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Run the detector on the image with the provided labels\n",
    "    results = detector(image, candidate_labels=labels_oxford_pets)\n",
    "    # Sort the results by score in descending order\n",
    "    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Get the top predicted label\n",
    "    predicted_label = sorted_results[0]['label']\n",
    "    \n",
    "    # Append the true and predicted labels to the respective lists\n",
    "    true_labels.append(dataset['train'][i]['label'])\n",
    "    predicted_labels.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', labels=labels_oxford_pets)\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', labels=labels_oxford_pets)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load models\n",
    "vit_classifier = pipeline(\"image-classification\", model=\"kuhs/vit-base-oxford-iiit-pets\")\n",
    "clip_detector = pipeline(model=\"openai/clip-vit-large-patch14\", task=\"zero-shot-image-classification\")\n",
    "\n",
    "labels_oxford_pets = [\n",
    "    'Siamese', 'Birman', 'shiba inu', 'staffordshire bull terrier', 'basset hound', 'Bombay', 'japanese chin',\n",
    "    'chihuahua', 'german shorthaired', 'pomeranian', 'beagle', 'english cocker spaniel', 'american pit bull terrier',\n",
    "    'Ragdoll', 'Persian', 'Egyptian Mau', 'miniature pinscher', 'Sphynx', 'Maine Coon', 'keeshond', 'yorkshire terrier',\n",
    "    'havanese', 'leonberger', 'wheaten terrier', 'american bulldog', 'english setter', 'boxer', 'newfoundland', 'Bengal',\n",
    "    'samoyed', 'British Shorthair', 'great pyrenees', 'Abyssinian', 'pug', 'saint bernard', 'Russian Blue', 'scottish terrier'\n",
    "]\n",
    "\n",
    "def classify_pet(image):\n",
    "    vit_results = vit_classifier(image)\n",
    "    vit_output = {result['label']: result['score'] for result in vit_results}\n",
    "    \n",
    "    clip_results = clip_detector(image, candidate_labels=labels_oxford_pets)\n",
    "    clip_output = {result['label']: result['score'] for result in clip_results}\n",
    "    \n",
    "    return {\"ViT Classification\": vit_output, \"CLIP Zero-Shot Classification\": clip_output}\n",
    "\n",
    "example_images = [\n",
    "    [\"example_images/dog1.jpeg\"],\n",
    "    [\"example_images/dog2.jpeg\"],\n",
    "    [\"example_images/leonberger.jpg\"],\n",
    "    [\"example_images/snow_leopard.jpeg\"],\n",
    "    [\"example_images/cat.jpg\"]\n",
    "]\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=classify_pet,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.JSON(),\n",
    "    title=\"Pet Classification Comparison\",\n",
    "    description=\"Upload an image of a pet, and compare results from a trained ViT model and a zero-shot CLIP model.\",\n",
    "    examples=example_images\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
